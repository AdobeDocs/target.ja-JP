---
keywords: AB;A/B;AB...n;エラー;落とし穴;ミス;落とし穴;有意性;勝者;統計的差異;統計的検出力;トラフィック配分;配分;
description: 企業が Adobe  [!DNL Target]  やその他のテストソリューションで A/B テストを実行する際に発生する、最も一般的な落とし穴やミスを回避する方法を説明します。
title: 一般的な A/B テストのエラーを回避する方法を教えてください。
feature: A/B Tests
exl-id: db085819-1a85-4936-bdc9-7501cf9b26ce
source-git-commit: 293b2869957c2781be8272cfd0cc9f82d8e4f0f0
workflow-type: ht
source-wordcount: '3898'
ht-degree: 100%

---

# A/B テストの一般的な 10 の落とし穴と回避方法

[!DNL Adobe Target] フォームでの A/B テストは、ほとんどのデジタルマーケティング最適化プログラムの根幹をなすもので、マーケティング担当者が最適化され、ターゲット設定されたエクスペリエンスを訪問者および顧客に提供するのを支援します。ここでは、A/B テストを実行する際に会社が陥る 10 個の最も重大な落とし穴について説明します。また、回避方法についても説明し、会社がテストの取り組みを通じてより優れた ROI を達成し、レポートされた A/B テストの結果の信頼性が高まります。

## 落とし穴 1：有意水準の効果を無視する {#section_55F5577A13C6470BA1417C2B735C6B1D}

2 つのオファーについて、実際にはコンバージョン率に違いがないにもかかわらず、有意な違いがあると報告される可能性はどれくらいあるでしょうか。テストの「*有意水準*」がその判定に役立ちます。このような誤解を与える検査結果は、偽陽性と呼ばれ、統計の世界では、第一種過誤とも呼ばれます（本当は真の帰無仮説を間違って棄却する場合）。

A/B テストの有意水準を指定すると、あるエクスペリエンスが実際とは違い、別のエクスペリエンスよりも優れていると判断されること（第一種過誤または「偽陽性」）に対する許容度と、実際には差異があるのに、エクスペリエンス間に統計的差異がないと見なすこと（第二種過誤または「偽陰性」）に対する許容度との間で、トレードオフを行うことになります。*信頼水準*&#x200B;はテストの前に決定されます。

テストの完了後に決定される&#x200B;*信頼区間*&#x200B;は、主にテストのサンプルサイズ、有意水準、母標準偏差の 3 つが関係します。有意水準はマーケティング担当者がテストの作成前に決めます。母標準偏差は調整できません。そのため、「調節可能な」要素はサンプルサイズのみです。適切な信頼区間の決定に必要なサンプルサイズと、そのサンプルサイズに達するまでの時間は、マーケティング担当者がテストの作成時に決める必要がある重要な要素です。

もう 1 つの直接関係する用語、「*信頼水準*」は、より楽観的です。信頼水準は、有意水準のように偽陽性の可能性を示すではなく、テストが誤りを犯さない可能性を示します。

以下に示すように、信頼水準と有意水準は、直接的に関連しています。

100％ - 信頼水準 = 有意水準

A/B テストでは、マーケティング担当者は、多くの場合、95％信頼水準を使用します。前述の方程式に基づいて、5％の有意水準に対応します。95％信頼水準のテストとは、実際にはオファー間に違いがない場合でも、統計的に有意な上昇率が 5％の確率で見つかるという意味です。

以下のグラフに示すように、テストを実行すればするほど、それらのテストのうちの少なくとも 1 つが偽陽性になる可能性が高まります。例えば、95％信頼水準を使用して 10 回テストを実行すると、約 40％の確率で 1 つまたは複数の偽陽性が見つかります（実際には上昇が見られないとすると：Pr（少なくとも 1 つの偽陽性）= 1 - Pr（偽陽性なし）= 1 - 0.95^10 = 40％）。

![pitfalls1 画像](assets/pitfalls1.png)

マーケティング組織では、95％は、通常、偽陽性と偽陰性のリスクとの適切なトレードオフとなります。

ただし、テスト後のセグメント化と複数オファーのテストという 2 つの状況のために、有意水準とテスト結果への影響に細心の注意を払う必要があります。

* **テスト後のセグメント化：**&#x200B;マーケティング担当者は、多くの場合、A/B テストが完了した後で訪問者セグメントに基づいてテストの結果を切り刻みます。一般的なセグメントには、ブラウザータイプ、デバイスタイプ、地域、時刻、新規訪問者と再訪問者との比較が含まれます。この方法は、テスト後のセグメント化と呼ばれ、訪問者セグメントに関する優れたインサイトをもたらします。次に、マーケティング担当者は、これらのインサイトを使用して、より適切にターゲット設定され、より関連性が高く差別化されたコンテンツを作成できます。

   実際にはコンバージョン率に違いがない場合、セグメントをテストするたびに、偽陽性の確率が有意水準に等しくなります。そして、既に述べたように、テストを実行すればするほど、これらのテストのうち少なくとも 1 つの偽陽性を経験する可能性は高まります。要するに、各テスト後のセグメントは、個別のテストを意味します。5％の有意水準の場合、平均して、テスト後のセグメントを 20 回調査するごとに 1 つの偽陽性が発生します。上の図には、その可能性がどのように増加するかが示されています。

   既に述べたように、テストを実行すればするほど、これらのテストのうち少なくとも 1 つの偽陽性を経験する可能性は高まります。要するに、各テスト後のセグメントは個別のテストを意味し、偽陽性の可能性が高まります。セグメントが相関関係にある場合は、この可能性の向上はさらに重要度が高まります。

   単にテスト後のセグメント化をおこなわなければ良いかというとそうではなく、テスト後のセグメントは有益です。代わりに、テスト後のセグメント化によるこの累積的な偽陽性の問題を回避するために、テスト後のセグメントを識別したら、新しいテストとして行うものと見なします。もう 1 つの方法として、次に説明する、ボンフェローニ補正を適用することもできます。

* **複数オファーのテスト：**&#x200B;マーケティング担当者は、2 つ以上のオファー（またはエクスペリエンス）を相互にテストすることがよくあります。これが、A/B/n テストと呼ばれる A/B テストソリューションを見かけることがある理由です（n は同時にテストしているオファーの数）。

   前述のとおり、テストした&#x200B;*各*&#x200B;オファーは、偽陽性率が有意水準と等しくなることに留意することが重要です。さらに、1 つのテスト環境内で複数のオファーが相互に対抗している場合、複数のテストを効率的に実行しています。例えば、A/B/C/D/E テストで 5 つのオファーを比較する場合、制御オファーと B、制御オファーと C、制御オファーと D、制御オファーと E の 4 つの比較を効率的に形成します。95％の信頼水準の場合、偽陽性の確率の 5％ではなく、実際は 18.5％になります。2

   全体的な信頼水準を 95％に維持し、この問題を回避するには、ボンフェローニ補正というものを適用します。この補正を使用して、単に有意水準を比較数で割って、95％信頼水準を達成するために必要な有意水準を見つけ出します。

   ボンフェローニ補正を前述の例に適用すると、5％/4 = 1.25％の有意水準を使用することになります。これは、個別のテストの信頼水準 98.75％と同じです（100％ - 1.25％ = 98.75％）。この調整により、説明した例のように、4 つのテストがある場合の有効信頼水準を 95％に維持します。

## 落とし穴 2：統計的に有意な違いのない複数のオファーのテストの勝者を宣言する {#section_FA83977C71DB4F69B3D438AF850EF3B6}

複数のオファーのテストでは、マーケティング担当者は、多くの場合、勝者と 2 番の間に統計的に有意な違いがないとしても、最も高い上昇率を示すオファーをテストの勝者と宣言します。この状況は、代替オファー間の違いが代替オファーと制御オファーの間の違いよりも小さい場合に発生します。下の図に、この概念を示します。黒いエラーの線は、上昇率の95％信頼区間を示します。制御オファーに対する各オファーの真の上昇率は、信頼区間（エラーの線で示される範囲）内に含まれ得る 95％です。

![pitfalls2 画像](assets/pitfalls2.png)

オファー A および B は、テスト中に観測された最も高い上昇率を持ち、C の信頼区間は A または B の信頼区間と重なってさえいないので、オファー C が将来のテストでこれらのオファーよりも優れたパフォーマンスを示すことはなさそうです。ただし、オファー A がテスト中に観測された最も高い上昇率を持っていても、信頼区間が重なっているので、オファー B が将来のテストでより優れたパフォーマンスを発揮する可能性は十分にあります。

ここで覚えておくべきことは、オファー A および B の両方がテストの勝者と見なされる必要があるということです。

通常、代替オファーの真の相対的パフォーマンスを特定するのに十分な時間テストを実行することは実現不可能であり、多くの場合、代替オファー間のパフォーマンスの違いは小さすぎてコンバージョン率に十分な影響を与えられません。そのような場合、結果を同順位として解釈し、戦略やページの他の要素の配置など、他の考慮事項を用いて、実装するオファーを判定できます。複数のテストでは、2 つ以上の勝者を受け入れる必要があります。これは、場合によっては、Web サイト開発で取るべき方向性の可能性をかなり広げることになります。

最も高いコンバージョン率のオファーを特定したい場合、他のすべてのオファーに対してすべてのオファーを比較することになります。前述の例では、n = 5 つのオファーがあり、n（n-1）/2 回、または 5 &#42;（5-1）/2 = 10 回比較する必要があります。この場合、ボンフェローニ補正では、テストの有意水準が 5％/10 = 0.5％になることが求められます。これは 99.5％の信頼水準に対応します。ただし、そのような高い信頼水準には、不必要に長い時間テストを実行することが求められます。

## 落とし穴 3：統計的検出力の効果を無視する {#section_0D517079B7D547CCAA75F80981CBE12A}

統計的検出力は、テストがオファー間のコンバージョン率の実際の違いを検出する確率です。コンバージョンイベントの特性がランダムなので、または統計学者がいうように「確率論的」なので、長期的には 2 つのオファー間でコンバージョン率に実際の違いが存在する場合でも、テストで統計的に有意な違いが示されないかもしれません。それを不運または偶然といいます。コンバージョン率の真の違いの検出に失敗することは、偽陰性または第二種過誤と呼ばれます。

テストの有効性を決める要素は 2 つあります。1 つ目は、サンプルサイズ、つまり、テスト対象とする訪問者数です。2 つ目は、テストで検出するコンバージョン率の違いの大きさです。もしかするとこれは直感的なものかもしれませんが、大きなコンバージョン率の違いにのみ関心がある場合、テストで実際にそのような大きな違いを検出する確率がより高くなります。ちょうど、紙タオルの筒を覗いて、ハエではなくリビングルームで飼っている象を発見するような感じです。そうすると、検出したい違いが小さければ小さいほど、大きなサンプルサイズが必要となり、そのサンプルサイズに到達するための時間も長くなります。

今日のマーケティング担当者は、非常に多くのテストで検出力が不足しています。つまり、使用しているサンプルサイズが小さすぎます。これは、実際にコンバージョン率に大きな違いが存在する場合でさえ、真の陽性を検出する望みが薄いということを意味します。実際、検出力不足のテストを継続的に実行すると、偽陽性の数が真陽性の数に匹敵するか、または多数を占めることさえあり得ます。これは、多くの場合、サイトにどっちつかずの変更を加えたり（時間の浪費）、実際にコンバージョン率を減少させる変更を加えることにつながります。

![pitfalls3 画像](assets/pitfalls3.png)

テストが検出力不足になるのを回避するには、95％の信頼水準と 80％の統計的検出力を含んでいるのが検出力に優れたテストであるという一般的な標準を考慮します。そのようなテストは、95％の確率で偽陽性を回避し、80％の確率で偽陰性を回避します。

## ステップ 4：片側検定を使用する {#section_8BB136D1DD6341FA9772F4C31E9AA37C}

片側検定では、特定の有意水準で勝者を判定するために、オファー間のコンバージョン率のより小さい違いを観測する必要があります。これは、両側検定を使用するよりも早く、より多く勝者を判定できるので、魅力的に見えます。しかし、「ただほど高いものはない」ということわざのとおり、片側検定は高くつきます。

片側検定では、オファー B がオファー A より優れているかどうかをテストします。テストの方向は、テストを開始する前（統計学でいうところの「アプリオリ」）に決定される必要があります。つまり、B が A よりも優れていることをテストするか、A が B よりも優れていることをテストするかを、*テストを開始する前*&#x200B;に決める必要があります。ただし、A/B テストの結果を調査して B が A よりも優れていることが&#x200B;*わかってから*、違いが統計的に有意かどうかを確認するために片側検定を行うことに決めると、統計的テストの前提に反することになります。テストの前提に反するということは、信頼区間が信頼できず、テストが予測よりも高い偽陽性率になることを意味します。

片側検定を、既に決心している裁判官によるオファーの裁判と考えることもできます。片側検定では、それぞれのエクスペリエンスに等しい確率を与えてそれ自体を勝者だと証明するよりも、勝者オファーが何になるかをあらかじめ決定し、それを証明しようとします。片側検定は、あるオファーが他方よりも優れているかどうかにのみ関心があり、その逆には関心がないというめったにない状況でのみ使用する必要があります。片側検定の問題を回避するには、[!DNL Adobe Target] など、常に両側検定を使用する A/B テストソリューションを使用します。

## 落とし穴 5：テストの監視 {#section_EA42F8D5967B439284D863C46706A1BA}

マーケティング担当者は、テストで有意な結果を判定するまで、A/B テストを頻繁に監視します。結局のところ、統計的優位差を獲得した後にテストする理由は何でしょうか。

残念ながら、その答えはそれほど単純ではありません。結果の監視が進行を妨げるのではなく、テストの有効な統計的優位差に悪影響を与えるということがわかっています。実際に偽陽性の可能性が大幅に増加し、信頼区間を信用できないものにします。

これは、わかりにくく思われるかもしれません。結果の中間テストを調査するだけで、統計的有意性を失わせる原因となり得るといっているように聞こえます。それは、実際に起こっていることとは違います。以下の例で理由を説明します。

コンバージョン率が共に 10％の 2 つのオファーの 10,000 個のコンバージョンイベントをシミュレートしているとします。コンバージョン率が同じなので、2 つのオファーを相互にテストした場合、コンバージョン上昇率は検出しないはずです。95％信頼区間を使用すると、10,000 個すべての観測結果を収集した後に評価された場合、テスト結果は期待された 5％の偽陽性率になります。そのため、このテストを 100 回実行すると、平均して 5 つの偽陽性が得られます（実際は、この例では、2 つのオファー間のコンバージョン率に違いがないので、すべての陽性は偽陽性になります）。ただし、このテスト中に 10 回（つまり 1,000 個の観測結果ごとに）テストを評価すると、偽陽性率が 16％に跳ね上がることがわかります。テストの監視は、偽陽性のリスクを 3 倍以上にするのです。どうしてそのようなことが起こるのでしょうか。

これが起こる理由を理解するために、有意な結果が検出された場合と検出されなかった場合とで取られる行動の違いを考慮する必要があります。統計的に有意な結果が検出されると、テストは停止され、勝者が宣言されます。ただし、結果が統計的に有意でない場合は、テストの続行を許可します。この状況では、前向きな結果に大きく偏向するので、テストの有効有意水準にゆがみが生じます。

この問題を防ぐためには、十分な長さの実施期間を設定してからテストを開始する必要があります。テストが正しく実行されていることを確認するためにテスト中にテスト結果を調査するのはよいですが、必要な数の訪問者に到達するまで、結論を出したり、テストを停止したりしないでください。要するに、のぞき見をしないということです。

## 落とし穴 6：テストを早めに停止する {#section_DF01A97275E44CA5859D825E0DE2F49F}

テストの最初の数日でオファーの 1 つのパフォーマンスが他に比べてずっと優れている、または劣っている場合、テストを停止したくなります。ただし、観測結果の数が少ない場合、コンバージョン率は少ない訪問者数の平均なので、まったく偶然にプラスまたはマイナスの上昇が観測される可能性が高いです。テストでより多くのデータポイントを収集するに従って、コンバージョン率は真の長期的な値に近づきます。

以下の図に、長期間のコンバージョン率が同じ 5 つのオファーを示します。オファー B は、最初の 2,000 人の訪問者に対して低いコンバージョン率を示し、推定コンバージョン率が真の長期的な値に戻るまでに時間がかかります。

![pitfalls4 画像](assets/pitfalls4.png)

この現象は、「平均への回帰」として知られ、テストの最初の数日のパフォーマンスが良かったオファーが長期的にパフォーマンス水準を維持できない場合、失望につながる可能性があります。また、まったくの偶然でテストの最初の数日にパフォーマンスが低かったために優れたオファーを実装しないと、売上高の減少につながる可能性もあります。

テスト監視の落とし穴とほぼ同じで、この問題を回避する最善の方法は、テストを実行する前に適正な訪問者数を決定し、この数の訪問者がオファーに接するまでテストを実行することです。

## 落とし穴 7：テスト期間中にトラフィック配分を変更する {#allocation}

テスト期間中はトラフィック配分の割合を変更しないことをお勧めします。変更すると、データが正規化されるまでテスト結果が偏って見える可能性があるからです。
例えば、A/B テストアクティビティで、トラフィックの 80%がエクスペリエンス A（コントロール）に、トラフィックの 20%がエクスペリエンス B に割り当てられているとします。テスト期間中に、各エクスペリエンスの配分を 50%に変更します。数日後、エクスペリエンス B のトラフィックの配分を 100%に変更します。

このシナリオでは、ユーザーはどのようにエクスペリエンスに割り当てられますか？

エクスペリエンス B の配分分割を手動で 100%に変更した場合、最初にエクスペリエンス A（コントロール）に配分された訪問者は、最初に割り当てられたエクスペリエンス（エクスペリエンス A）のままとなります。トラフィック配分の変更は、新規参加者にのみ影響します。

割合を変更したり、各エクスペリエンスへの訪問者のフローに大きく影響を与えたりする場合は、新しいアクティビティを作成するか、アクティビティをコピーしてから、トラフィック配分の割合を編集することをお勧めします。

テスト期間中に様々なエクスペリエンスの割合を変すると、特に多くの購入者が再訪問者の場合に、データが正規化されるまで数日かかります。
もう一つの例として、A/B テストのトラフィック割合が 50/50 であり、この割合を後で 80/20 に変更した場合、変更後の最初の数日間は、結果が偏って見える場合があります。コンバージョンまでの平均時間が長い場合、つまり、訪問者が購買するまでに数時間または数日かかる場合は、これらのコンバージョンの遅れがレポートに影響を及ぼすことがあります。そのため、数値が 50% から 80% に変更され、コンバージョンまでの平均時間が 2 日である最初のエクスペリエンスでは、テストの初日にコンバージョンされる訪問者は 50％だけですが、今日は 80％の訪問者がエクスペリエンスに入ることになります。これによりコンバージョン率が急落しますが、これら 80％の訪問者が 2 日かけてコンバージョンされた後で再び通常に戻ります。

## 落とし穴 8：新奇性効果を考えていない {#section_90F0D24C40294A8F801B1A6D6DEF9003}

テストの実行に十分な時間を与えないと、別の予期しないことが発生する可能性があります。今度の問題は、統計学の問題ではなく、単純に訪問者による変更に対する反応です。Web サイトの安定した部分を変更する場合、通常のワークフローが変更されるので、再訪問者は、最初は新しいオファーへの関与が完全に減るかもしれません。これは、再訪問者が適切にその変化に慣れるまで、一時的に優れた新しいオファーのパフォーマンスが低くなる原因となる可能性があります。その期間は、優れたオファーによる長期的な利益を得るための、ほんの少しの代償です。

新しいオファーの低いパフォーマンスが新奇性効果によるものか、それとも本当に劣っているためなのかを判断するには、訪問者を新規と再訪問者でセグメント化し、コンバージョン率を比較します。単なる新奇性効果の場合、新しいオファーは新規訪問者で勝者になります。最終的に、再訪問者が新しい変更に慣れるにしたがって、オファーは再訪問者でも勝者になります。

また、新奇性効果は、逆にも作用します。訪問者は、多くの場合、単に何か新しいものが導入されるという理由で、変更に対して肯定的に反応します。しばらくして、新しいコンテンツが新鮮さを失ったり、訪問者への刺激が薄れると、コンバージョン率は下がります。この効果は、特定が難しいですが、コンバージョン率の変化を慎重に監視することが、これを検出するために重要です。

## 落とし穴 9：検討期間の違いを考えていない {#section_B166731B5BEE4E578816E351ECDEA992}

検討期間は、A/B テストソリューションが訪問者にオファーを提示してから訪問者がコンバージョンを行うまでの期間です。これは、検討期間に大幅に影響を与えるオファー（例えば、「期間限定オファー - 今週日曜日まで」のように期日を暗示するオファー）にとって重要です。

そうしたオファーは、訪問者により早いコンバージョンを促し、オファーの有効期限が切れるとすぐにテストが停止される場合に有利です。代替オファーは、期日がより長いか期限がなく、その結果、検討期間が長くなるためです。代替オファーでテスト終了後の期間にコンバージョンが発生しても、期日の終了時にテストを停止すると、それ以降のコンバージョンは、テストのコンバージョン率にカウントされません。

以下の図に、2 人の異なる訪問者が日曜日の午後に同時に表示した 2 つのオファーを示します。オファー A の検討期間は短く、訪問者はその日の後になってコンバージョンをおこないます。一方、オファー B は検討期間が長く、オファー B を表示した訪問者は、しばらくオファーを検討して、最終的に月曜日の朝にコンバージョンをおこないます。日曜日の夜にテストを停止すると、オファー A に関連付けられたコンバージョンは、オファー A のコンバージョン指標でカウントされるのに対して、オファー B に関連付けられたコンバージョンは、オファー B のコンバージョン指標でカウントされません。これによって、オファー B がはるかに不利になります。

![pitfalls5 画像](assets/pitfalls5.png)

この落とし穴を回避するには、テストオファーに触れた訪問者が、テストへの新規参入が停止された後にコンバージョンできるように時間を与えます。この手順により、オファーを公平に比較できます。

## 落とし穴 10：ビジネス目標を反映していない指標を使用する {#section_F0CD6DC7993B4A6F9BEEBB31CD1D9BEE}

マーケティング担当者は、テストコンバージョンの適正な数に早く到達するために、ファネルの上位では、クリックスルー率（CTR）などの高トラフィックで低分散のコンバージョン指標を使用したほうがよいという気になります。ただし、CTR が達成すべきビジネス目標の適切な代理であるかどうかを慎重に検討してください。高い CTR を持つオファーは、簡単に収益を減少させることにつながる可能性があります。これは、オファーが低購入傾向の訪問者を引き付ける場合、またはオファー自体が単純に収益を減少させることにつながるもの（例えば、割引オファー）である場合に発生します。

![pitfalls6 画像](assets/pitfalls6.png)

以下のスキーオファーについて検討しましょう。スキーオファーはサイクリングオファーよりもずっと高い CTR を生成しますが、訪問者はサイクリングオファーをフォローしている場合に平均してより多く支払っているので、特定の訪問者に提示されるサイクリングオファーの予測収益の方が高くなります。その結果、CTR を指標として使用した A/B テストは、基本的なビジネス目標である収益の最大化をおこなわないオファーを選択することになります。

![pitfalls7 画像](assets/pitfalls7.png)

この問題を回避するには、ビジネス指標を慎重に監視してビジネスへのオファーの影響を特定するか、いっそのこと、可能であれば、よりビジネス目標に近い指標を使用します。

## 結論：A/B テストの成功は、落とし穴を認識し、回避することによる {#section_54D33248163A481EBD4421A786FE2B15}

一般的な A/B テストの落とし穴について学んだので、いつどこでその落とし穴に陥る可能性があるかを特定できることを期待します。また、数学の学位を持つ人々の領分のように感じることも多い、A/B テストに関する統計および確率の概念の一部について、さらなる理解で武装していることを期待します。

以下の手順は、これらの落とし穴を回避し、A/B テストからより良い結果を得ることに注力するのに役立ちます。

* 関連するビジネス目標に基づいて、テストに適した指標を慎重に検討する。
* テストを開始する前に信頼水準を決定し、テスト終了後に結果を評価する際に、このしきい値に従う。
* テストが開始される前に、サンプルサイズ（訪問者数）を計算する。
* 計算したサンプルサイズに到達するまで、テストの停止を待つ。
* テスト後のセグメント化を行う際や複数の代替オファーを評価する際には、例えば、ボンフェローニ補正を使用して、信頼水準を調整する。
